{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eSzMrsZBhA8pcwCRkGOAHYwtkZHoxbZt","timestamp":1760261672592},{"file_id":"1_11PB5-pQp7vtEdNBROMeALVBZQEomRU","timestamp":1759424004270},{"file_id":"1dWR-PFFxXOWek0buOgKOfYaJJvwM1e6p","timestamp":1759409631871},{"file_id":"1O3cOXgmr5nE5qmFZwJmYtxkrMriN62Jv","timestamp":1759215418576}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMysAOjwLalbKYknwS6R1lB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ─────────────────────────────────────────────────────────────\n","# 0. Imports\n","# ─────────────────────────────────────────────────────────────\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    accuracy_score, precision_score, recall_score, f1_score\n",")\n","from sklearn.preprocessing import LabelEncoder, normalize\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.tree import DecisionTreeClassifier\n","from sentence_transformers import SentenceTransformer\n","import torch\n","import time\n","from google.colab import drive\n","\n","# ─────────────────────────────────────────────────────────────\n","# 1. Mount Google Drive\n","# ─────────────────────────────────────────────────────────────\n","drive.mount('/content/drive')\n","\n","# ─────────────────────────────────────────────────────────────\n","# 2. Load UNSW-NB15 train/test\n","# ─────────────────────────────────────────────────────────────\n","unsw_train_path = \"/content/drive/MyDrive/Research Project/UNSW_NB15_training-set.csv\"\n","unsw_test_path  = \"/content/drive/MyDrive/Research Project/UNSW_NB15_testing-set.csv\"\n","\n","unsw_train = pd.read_csv(unsw_train_path)\n","unsw_test  = pd.read_csv(unsw_test_path)\n","\n","print(\"UNSW-NB15 columns:\", list(unsw_train.columns)[:10], \"... total:\", len(unsw_train.columns))\n","\n","# ─────────────────────────────────────────────────────────────\n","# 3. Binary labels\n","# ─────────────────────────────────────────────────────────────\n","if \"label\" not in unsw_train.columns:\n","    def _binlab(df):\n","        if \"attack_cat\" in df.columns:\n","            return (df[\"attack_cat\"].astype(str).str.lower() != \"normal\").astype(int)\n","        raise ValueError(\"Missing 'label' or 'attack_cat'\")\n","    unsw_train[\"label\"] = _binlab(unsw_train)\n","    unsw_test[\"label\"]  = _binlab(unsw_test)\n","else:\n","    unsw_train[\"label\"] = (unsw_train[\"label\"] != 0).astype(int)\n","    unsw_test[\"label\"]  = (unsw_test[\"label\"]  != 0).astype(int)\n","\n","# ─────────────────────────────────────────────────────────────\n","# 4. Feature engineering\n","# ─────────────────────────────────────────────────────────────\n","for df in [unsw_train, unsw_test]:\n","    if \"sbytes\" in df.columns and \"dbytes\" in df.columns:\n","        df[\"bytes_ratio\"] = df[\"sbytes\"] / (df[\"dbytes\"] + 1)\n","        df[\"log_sbytes\"] = np.log1p(df[\"sbytes\"])\n","        df[\"log_dbytes\"] = np.log1p(df[\"dbytes\"])\n","    if \"dur\" in df.columns:\n","        df[\"log_dur\"] = np.log1p(df[\"dur\"])\n","    if \"spkts\" in df.columns and \"dpkts\" in df.columns:\n","        df[\"pkt_ratio\"] = df[\"spkts\"] / (df[\"dpkts\"] + 1)\n","\n","# ─────────────────────────────────────────────────────────────\n","# 5. Encode categorical features\n","# ─────────────────────────────────────────────────────────────\n","categorical_cols = [c for c in [\"proto\", \"service\", \"state\"] if c in unsw_train.columns]\n","for col in categorical_cols:\n","    le = LabelEncoder()\n","    combined = pd.concat([unsw_train[col].astype(str), unsw_test[col].astype(str)], axis=0)\n","    le.fit(combined)\n","    unsw_train[col] = le.transform(unsw_train[col].astype(str))\n","    unsw_test[col]  = le.transform(unsw_test[col].astype(str))\n","\n","# ─────────────────────────────────────────────────────────────\n","# 6. Numeric columns\n","# ─────────────────────────────────────────────────────────────\n","drop_cols = [\"srcip\", \"dstip\", \"label\", \"attack_cat\"]\n","num_cols = [c for c in unsw_train.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(unsw_train[c])]\n","\n","X_train_num = unsw_train[num_cols].values\n","X_test_num  = unsw_test[num_cols].values\n","y_train = unsw_train[\"label\"].values\n","y_test  = unsw_test[\"label\"].values\n","\n","# ─────────────────────────────────────────────────────────────\n","# 7. Flow summaries\n","# ─────────────────────────────────────────────────────────────\n","def make_summary(row):\n","    g = row.get\n","    srcbytes = g(\"sbytes\", g(\"srcbytes\", 0))\n","    dstbytes = g(\"dbytes\", g(\"dstbytes\", 0))\n","    dur      = g(\"dur\", 0.0)\n","    proto    = g(\"proto\", \"NA\")\n","    service  = g(\"service\", \"NA\")\n","    state    = g(\"state\", \"NA\")\n","    sport    = g(\"sport\", g(\"sport\", -1))\n","    dsport   = g(\"dsport\", g(\"dsport\", -1))\n","    return (f\"Flow: src_bytes={srcbytes}, dst_bytes={dstbytes}, \"\n","            f\"duration={float(dur):.2f}s, proto={proto}, service={service}, \"\n","            f\"state={state}, sport={sport}, dsport={dsport}\")\n","\n","train_summaries = unsw_train.apply(make_summary, axis=1).tolist()\n","test_summaries  = unsw_test.apply(make_summary, axis=1).tolist()\n","\n","# ─────────────────────────────────────────────────────────────\n","# 8. Load BGE model\n","# ─────────────────────────────────────────────────────────────\n","print(\"\\nLoading BGE model...\")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","bge_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n","\n","# ─────────────────────────────────────────────────────────────\n","# 9. Compute embeddings\n","# ─────────────────────────────────────────────────────────────\n","def compute_bge_embeddings(texts, batch_size=32):\n","    all_embs = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding with Mini_LM\"):\n","        batch = texts[i:i+batch_size]\n","        emb = bge_model.encode(batch, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)\n","        all_embs.append(emb)\n","    return np.vstack(all_embs)\n","\n","t0 = time.perf_counter()\n","train_emb = compute_bge_embeddings(train_summaries)\n","t1 = time.perf_counter()\n","print(f\"Train embedding time/sample: {(t1 - t0)/len(train_summaries)*1000:.2f} ms\")\n","\n","t0 = time.perf_counter()\n","test_emb = compute_bge_embeddings(test_summaries)\n","t1 = time.perf_counter()\n","print(f\"Test embedding time/sample: {(t1 - t0)/len(test_summaries)*1000:.2f} ms\")\n","\n","# ─────────────────────────────────────────────────────────────\n","# 10. Combine numeric + MiniLM embeddings\n","# ─────────────────────────────────────────────────────────────\n","X_train = np.hstack([X_train_num, train_emb])\n","X_test  = np.hstack([X_test_num, test_emb])\n","print(\"Final training shape:\", X_train.shape)\n","\n","# ─────────────────────────────────────────────────────────────\n","# 11. Train & Evaluate Decision Tree\n","# ─────────────────────────────────────────────────────────────\n","clf = DecisionTreeClassifier(\n","    criterion=\"entropy\",\n","    max_depth=10,\n","    min_samples_leaf=5,\n","    random_state=42\n",")\n","\n","print(\"\\nTraining Decision Trees model...\")\n","t0 = time.perf_counter()\n","clf.fit(X_train, y_train)\n","t1 = time.perf_counter()\n","print(f\"Training time: {(t1 - t0):.2f}s\")\n","\n","# ─────────────────────────────────────────────────────────────\n","# 12. Evaluation\n","# ─────────────────────────────────────────────────────────────\n","y_pred = clf.predict(X_test)\n","print(\"\\n Performance Report (MiniLM + Decision Trees):\")\n","print(f\"Accuracy : {accuracy_score(y_test, y_pred):.3f}\")\n","print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n","print(f\"Recall   : {recall_score(y_test, y_pred):.3f}\")\n","print(f\"F1 Score : {f1_score(y_test, y_pred):.3f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"normal\", \"attack\"]))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BvJBnyaG24Vp","executionInfo":{"status":"ok","timestamp":1760262108453,"user_tz":-630,"elapsed":141595,"user":{"displayName":"Tuhin Anand","userId":"00460084127537747763"}},"outputId":"64cc9ef1-af49-4335-8e5f-5cffeef62d4f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","UNSW-NB15 columns: ['id', 'dur', 'proto', 'service', 'state', 'spkts', 'dpkts', 'sbytes', 'dbytes', 'rate'] ... total: 45\n","\n","Loading BGE model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","Embedding with Mini_LM: 100%|██████████| 5480/5480 [00:52<00:00, 103.96it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Train embedding time/sample: 0.30 ms\n"]},{"output_type":"stream","name":"stderr","text":["Embedding with Mini_LM: 100%|██████████| 2573/2573 [00:24<00:00, 106.99it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Test embedding time/sample: 0.29 ms\n","Final training shape: (175341, 432)\n","\n","Training Extra Trees model...\n","Training time: 38.12s\n","\n"," Performance Report (MiniLM + ExtraTrees):\n","Accuracy : 0.519\n","Precision: 0.603\n","Recall   : 0.368\n","F1 Score : 0.457\n","\n","Classification Report:\n","               precision    recall  f1-score   support\n","\n","      normal       0.48      0.70      0.57     37000\n","      attack       0.60      0.37      0.46     45332\n","\n","    accuracy                           0.52     82332\n","   macro avg       0.54      0.54      0.51     82332\n","weighted avg       0.55      0.52      0.51     82332\n","\n","Confusion Matrix:\n"," [[26005 10995]\n"," [28635 16697]]\n"]}]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","\n","def evaluate_model(clf, X_test, y_test):\n","    import time\n","    start = time.perf_counter()\n","    y_pred = clf.predict(X_test)\n","    end = time.perf_counter()\n","    cls_latency = (end - start) / len(y_test) * 1000  # ms per sample\n","\n","    y_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, \"predict_proba\") else None\n","\n","    # Confusion matrix + FPR\n","    cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n","    tn, fp, fn, tp = cm.ravel()\n","    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n","\n","    results = {\n","        \"Accuracy\": accuracy_score(y_test, y_pred),\n","        \"Precision\": precision_score(y_test, y_pred),\n","        \"Recall\": recall_score(y_test, y_pred),\n","        \"F1\": f1_score(y_test, y_pred),\n","        \"ROC-AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n","        \"FPR\": fpr,\n","        \"ConfusionMatrix\": cm.tolist(),\n","        \"Latency_cls_ms\": cls_latency\n","    }\n","    return results\n"],"metadata":{"id":"OXUu3DinXiKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","def save_confusion_matrix(cm, run_name, out_dir=\"/content/drive/MyDrive/Results/UNSW/DecisionTree\"):\n","    os.makedirs(out_dir, exist_ok=True)\n","    # Save CSV\n","    np.savetxt(os.path.join(out_dir, f\"{run_name}_cm.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n","\n","    # Save image\n","    fig, ax = plt.subplots(figsize=(4,4))\n","    im = ax.imshow(cm, cmap=\"Blues\")\n","    ax.set_title(f\"Confusion Matrix - {run_name}\")\n","    ax.set_xlabel(\"Predicted\")\n","    ax.set_ylabel(\"True\")\n","    ax.set_xticks([0,1]); ax.set_xticklabels([\"Normal\",\"Attack\"])\n","    ax.set_yticks([0,1]); ax.set_yticklabels([\"Normal\",\"Attack\"])\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"red\")\n","\n","    fig.colorbar(im, ax=ax)\n","    fig.tight_layout()\n","    fig.savefig(os.path.join(out_dir, f\"{run_name}_cm.png\"))\n","    plt.close(fig)\n"],"metadata":{"id":"s5q9Gw2tXjK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","results = evaluate_model(clf, X_test, y_test)\n","\n","cm = np.array(results[\"ConfusionMatrix\"])\n","save_confusion_matrix(cm, run_name=\"MiniLM_DT\")\n","\n","with open(\"/content/drive/MyDrive/Results/UNSW/DecisionTree/MiniLM_Results.txt\", \"w\") as f:\n","    json.dump(results, f, indent=2)\n"],"metadata":{"id":"wdmSJ4hbXv8c"},"execution_count":null,"outputs":[]}]}