{"cells":[{"cell_type":"code","source":["import os\n","import numpy as np\n","import pandas as pd\n","from transformers import GPT2Tokenizer, GPT2Model\n","from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n","from torch.utils.data import Dataset, DataLoader\n","import torch\n","import time\n","from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"L_oZHNfOJn1E"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJNOQI3vfvbF"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# 1. Load NSL-KDD Dataset\n","# ──────────────────────────────────────────────────────────────────────────────\n","train_df = pd.read_csv(\"/content/drive/MyDrive/Research Project/UNSW_NB15_training-set.csv\")\n","test_df  = pd.read_csv(\"/content/drive/MyDrive/Research Project/UNSW_NB15_testing-set.csv\")"],"metadata":{"id":"QAiV9hDyH36M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"UNSW-NB15 columns:\", list(train_df.columns)[:10], \"... (total:\", len(train_df.columns), \")\")\n","\n","# ──────────────────────────────────────────────────────────────────────────────\n","# B. Ensure binary label\n","# ──────────────────────────────────────────────────────────────────────────────\n","if \"label\" not in train_df.columns:\n","    def _binlab(df):\n","        if \"attack_cat\" in df.columns:\n","            return (df[\"attack_cat\"].astype(str).str.lower() != \"normal\").astype(int)\n","        raise ValueError(\"UNSW-NB15 needs either 'label' or 'attack_cat' to build a binary label.\")\n","    train_df[\"label\"] = _binlab(train_df)\n","    test_df[\"label\"]  = _binlab(test_df)\n","else:\n","    # Make sure it's 0/1 ints\n","    train_df[\"label\"] = (train_df[\"label\"] != 0).astype(int)\n","    test_df[\"label\"]  = (test_df[\"label\"]  != 0).astype(int)"],"metadata":{"id":"CY8cRWEHEz7Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# 2. Numeric Features + Flow Summaries\n","# ──────────────────────────────────────────────────────────────────────────────\n","categorical_cols_unsw = [c for c in [\"proto\",\"service\",\"state\",\"attack_cat\",\"srcip\",\"dstip\"]\n","                         if c in train_df.columns]\n","drop_cols_unsw = categorical_cols_unsw + [\"label\"]\n","\n","num_cols_unsw = [c for c in train_df.columns\n","                 if c not in drop_cols_unsw and pd.api.types.is_numeric_dtype(train_df[c])]\n","\n","X_train_num = train_df[num_cols_unsw].values\n","X_test_num  = test_df[num_cols_unsw].values\n","y_train     = train_df[\"label\"].values\n","y_test      = test_df[\"label\"].values\n"],"metadata":{"id":"Pt7RPeuZH8Ux"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# D. Flow summaries for UNSW\n","# ──────────────────────────────────────────────────────────────────────────────\n","def make_summary(row):\n","    g = row.get\n","    srcbytes = g(\"sbytes\", g(\"srcbytes\", 0))\n","    dstbytes = g(\"dbytes\", g(\"dstbytes\", 0))\n","    dur      = g(\"dur\", 0.0)\n","    proto    = g(\"proto\", \"NA\")\n","    service  = g(\"service\", \"NA\")\n","    state    = g(\"state\", \"NA\")\n","    sport    = g(\"sport\", g(\"sport\", -1))\n","    dsport   = g(\"dsport\", g(\"dsport\", -1))\n","\n","    return (f\"Flow: src_bytes={srcbytes}, dst_bytes={dstbytes}, \"\n","            f\"duration={float(dur):.2f}s, proto={proto}, service={service}, \"\n","            f\"state={state}, sport={sport}, dsport={dsport}\")\n","\n","train_summaries = train_df.apply(make_summary, axis=1).tolist()\n","test_summaries  = test_df.apply(make_summary, axis=1).tolist()"],"metadata":{"id":"wrH0vgBkFGjC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# 3. GPT-2 Tokenizer and Model\n","# ──────────────────────────────────────────────────────────────────────────────\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n","tokenizer.pad_token = tokenizer.eos_token\n","\n","model = GPT2Model.from_pretrained(\"gpt2\", output_hidden_states=True)\n","model.to(device).eval()"],"metadata":{"id":"V8a88t30ICZB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# 4. Embedding Function\n","# ──────────────────────────────────────────────────────────────────────────────\n","class SummaryDataset(Dataset):\n","    def __init__(self, summaries):\n","        self.summaries = summaries\n","    def __len__(self):\n","        return len(self.summaries)\n","    def __getitem__(self, idx):\n","        return self.summaries[idx]\n","\n","def collate_fn(batch):\n","    return tokenizer(\n","        batch,\n","        return_tensors=\"pt\",\n","        padding=True,\n","        truncation=True,\n","        max_length=128\n","    )\n","\n","def get_embeddings(summaries, batch_size=32):\n","    ds = SummaryDataset(summaries)\n","    loader = DataLoader(ds, batch_size=batch_size, collate_fn=collate_fn)\n","    all_embs = []\n","    with torch.no_grad():\n","        for batch in loader:\n","            batch = {k: v.to(device) for k, v in batch.items()}\n","            outputs = model(**batch)\n","            last_hidden = outputs.hidden_states[-1]\n","            pooled = last_hidden.mean(dim=1).cpu().numpy()\n","            all_embs.append(pooled)\n","    return np.vstack(all_embs)"],"metadata":{"id":"3MRTHIhGIGPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Training embeddings timing\n","print(\"Generating GPT-2 embeddings for training set...\")\n","t0 = time.perf_counter()\n","train_emb = get_embeddings(train_summaries)\n","t1 = time.perf_counter()\n","embed_time_train = (t1 - t0) / len(train_summaries) * 1000\n","\n","# Test embeddings timing\n","print(\"Generating GPT-2 embeddings for test set...\")\n","t0 = time.perf_counter()\n","test_emb  = get_embeddings(test_summaries)\n","t1 = time.perf_counter()\n","embed_time_test = (t1 - t0) / len(test_summaries) * 1000\n","\n","print(f\"Embedding time per sample (train): {embed_time_train:.2f} ms\")\n","print(f\"Embedding time per sample (test): {embed_time_test:.2f} ms\")"],"metadata":{"id":"u4YFHwj8dnqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# 5. Combine Numeric + GPT Embeddings\n","# ──────────────────────────────────────────────────────────────────────────────\n","X_train = np.hstack([X_train_num, train_emb])\n","X_test  = np.hstack([X_test_num,  test_emb])"],"metadata":{"id":"EnMWVLUMIMEk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# 6. Train Random Forest Classifier\n","# ──────────────────────────────────────────────────────────────────────────────\n","clf = RandomForestClassifier(\n","    n_estimators=200,\n","    max_depth=None,\n","    min_samples_leaf=5,\n","    n_jobs=-1,\n","    random_state=42\n",")\n","clf.fit(X_train, y_train)"],"metadata":{"id":"OgUwxOjDISKK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ──────────────────────────────────────────────────────────────────────────────\n","# 7. Evaluation\n","# ──────────────────────────────────────────────────────────────────────────────\n","y_pred = clf.predict(X_test)\n","\n","print(\"\\nPerformance Report (GPT-2 + Random Forest):\")\n","print(f\"Accuracy : {accuracy_score(y_test, y_pred):.3f}\")\n","print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n","print(f\"Recall   : {recall_score(y_test, y_pred):.3f}\")\n","print(f\"F1 Score : {f1_score(y_test, y_pred):.3f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"normal\", \"attack\"]))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"],"metadata":{"id":"bkep83HeIT1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","\n","def evaluate_model(clf, X_test, y_test):\n","    import time\n","    start = time.perf_counter()\n","    y_pred = clf.predict(X_test)\n","    end = time.perf_counter()\n","    cls_latency = (end - start) / len(y_test) * 1000\n","\n","    y_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, \"predict_proba\") else None\n","\n","    # Confusion matrix\n","    cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n","    tn, fp, fn, tp = cm.ravel()\n","    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n","\n","    results = {\n","        \"Accuracy\": accuracy_score(y_test, y_pred),\n","        \"Precision\": precision_score(y_test, y_pred),\n","        \"Recall\": recall_score(y_test, y_pred),\n","        \"F1\": f1_score(y_test, y_pred),\n","        \"ROC-AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n","        \"FPR\": fpr,\n","        \"ConfusionMatrix\": cm.tolist(),\n","        \"Latency_cls_ms\": cls_latency\n","    }\n","    return results\n"],"metadata":{"id":"HMlnD8iZYGgQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","results = evaluate_model(clf, X_test, y_test)\n","with open(\"/content/drive/MyDrive/Results/UNSW/Random Forest/GPT2_Results.json\", \"w\") as f:\n","    json.dump(results, f, indent=2)\n"],"metadata":{"id":"bwBXQPfzYHQd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","def save_confusion_matrix(cm, run_name, out_dir=\"/content/drive/MyDrive/Results/UNSW/Random Forest\"):\n","    os.makedirs(out_dir, exist_ok=True)\n","\n","    np.savetxt(os.path.join(out_dir, f\"{run_name}_cm.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n","\n","    fig, ax = plt.subplots(figsize=(4,4))\n","    im = ax.imshow(cm, cmap=\"Blues\")\n","    ax.set_title(f\"Confusion Matrix - {run_name}\")\n","    ax.set_xlabel(\"Predicted\")\n","    ax.set_ylabel(\"True\")\n","    ax.set_xticks([0,1]); ax.set_xticklabels([\"Normal\",\"Attack\"])\n","    ax.set_yticks([0,1]); ax.set_yticklabels([\"Normal\",\"Attack\"])\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"red\")\n","\n","    fig.colorbar(im, ax=ax)\n","    fig.tight_layout()\n","    fig.savefig(os.path.join(out_dir, f\"{run_name}_cm.png\"))\n","    plt.close(fig)\n"],"metadata":{"id":"nuW_2ypNh8Zx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results = evaluate_model(clf, X_test, y_test)\n","\n","cm = np.array(results[\"ConfusionMatrix\"])\n","save_confusion_matrix(cm, run_name=\"GPT2_DT\")\n","\n","with open(\"/content/drive/MyDrive/Results/UNSW/Random Forest/GPT2_Results.json\", \"w\") as f:\n","    json.dump(results, f, indent=2)\n"],"metadata":{"id":"JykZE2BXiAXx"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1iion8E7Qvrctk3fKoiPljwOOdGANcr4n","timestamp":1759411027946},{"file_id":"14pVW0uS06LOoycAYZQSazFXgF674tmju","timestamp":1759215431349}],"gpuType":"A100","authorship_tag":"ABX9TyN03cL85vztLee8xlj8VU/s"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}