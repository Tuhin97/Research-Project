{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tIdMnpzs2isV"
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 0. Imports\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from imblearn.combine import SMOTEENN\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import time\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1. Mount Google Drive\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2. Load UNSW-NB15 train/test\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "unsw_train_path = \"/content/drive/MyDrive/Research Project/UNSW_NB15_training-set.csv\"\n",
    "unsw_test_path  = \"/content/drive/MyDrive/Research Project/UNSW_NB15_testing-set.csv\"\n",
    "\n",
    "unsw_train = pd.read_csv(unsw_train_path)\n",
    "unsw_test  = pd.read_csv(unsw_test_path)\n",
    "\n",
    "print(\"UNSW-NB15 columns:\", list(unsw_train.columns)[:10], \"... total:\", len(unsw_train.columns))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3. Binary labels\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if \"label\" not in unsw_train.columns:\n",
    "    def _binlab(df):\n",
    "        if \"attack_cat\" in df.columns:\n",
    "            return (df[\"attack_cat\"].astype(str).str.lower() != \"normal\").astype(int)\n",
    "        raise ValueError(\"Missing 'label' or 'attack_cat'\")\n",
    "    unsw_train[\"label\"] = _binlab(unsw_train)\n",
    "    unsw_test[\"label\"]  = _binlab(unsw_test)\n",
    "else:\n",
    "    unsw_train[\"label\"] = (unsw_train[\"label\"] != 0).astype(int)\n",
    "    unsw_test[\"label\"]  = (unsw_test[\"label\"]  != 0).astype(int)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4. Enhanced feature engineering\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for df in [unsw_train, unsw_test]:\n",
    "    if \"sbytes\" in df.columns and \"dbytes\" in df.columns:\n",
    "        df[\"bytes_ratio\"] = df[\"sbytes\"] / (df[\"dbytes\"] + 1)\n",
    "        df[\"log_sbytes\"] = np.log1p(df[\"sbytes\"])\n",
    "        df[\"log_dbytes\"] = np.log1p(df[\"dbytes\"])\n",
    "    if \"dur\" in df.columns:\n",
    "        df[\"log_dur\"] = np.log1p(df[\"dur\"])\n",
    "    if \"spkts\" in df.columns and \"dpkts\" in df.columns:\n",
    "        df[\"pkt_ratio\"] = df[\"spkts\"] / (df[\"dpkts\"] + 1)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5. Encode categorical features safely\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "categorical_cols = [c for c in [\"proto\", \"service\", \"state\"] if c in unsw_train.columns]\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([unsw_train[col].astype(str), unsw_test[col].astype(str)], axis=0)\n",
    "    le.fit(combined)\n",
    "    unsw_train[col] = le.transform(unsw_train[col].astype(str))\n",
    "    unsw_test[col]  = le.transform(unsw_test[col].astype(str))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6. Select numeric columns\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "drop_cols = [\"srcip\", \"dstip\", \"label\", \"attack_cat\"]\n",
    "num_cols = [c for c in unsw_train.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(unsw_train[c])]\n",
    "\n",
    "X_train_num = unsw_train[num_cols].values\n",
    "X_test_num  = unsw_test[num_cols].values\n",
    "y_train = unsw_train[\"label\"].values\n",
    "y_test  = unsw_test[\"label\"].values\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7. Text summarization for Gemma embeddings\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def make_summary(row):\n",
    "    g = row.get\n",
    "    srcbytes = g(\"sbytes\", g(\"srcbytes\", 0))\n",
    "    dstbytes = g(\"dbytes\", g(\"dstbytes\", 0))\n",
    "    dur      = g(\"dur\", 0.0)\n",
    "    proto    = g(\"proto\", \"NA\")\n",
    "    service  = g(\"service\", \"NA\")\n",
    "    state    = g(\"state\", \"NA\")\n",
    "    sport    = g(\"sport\", g(\"sport\", -1))\n",
    "    dsport   = g(\"dsport\", g(\"dsport\", -1))\n",
    "    return (f\"Flow: src_bytes={srcbytes}, dst_bytes={dstbytes}, \"\n",
    "            f\"duration={float(dur):.2f}s, proto={proto}, service={service}, \"\n",
    "            f\"state={state}, sport={sport}, dsport={dsport}\")\n",
    "\n",
    "train_summaries = unsw_train.apply(make_summary, axis=1).tolist()\n",
    "test_summaries  = unsw_test.apply(make_summary, axis=1).tolist()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 8. Load Gemma model\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "login(token=\"your token hugging face\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model_name = \"google/gemma-3-270m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True, output_hidden_states=True).to(device)\n",
    "model.eval()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 9. Embedding dataset + improved pooling\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, summaries):\n",
    "        self.summaries = summaries\n",
    "    def __len__(self):\n",
    "        return len(self.summaries)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.summaries[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "def compute_embeddings(summaries, batch_size=32):\n",
    "    ds = SummaryDataset(summaries)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    all_embs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Embedding with Gemma\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "            pooled = last_hidden[:, 0, :].cpu().numpy()\n",
    "            all_embs.append(pooled)\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 10. Compute & normalize embeddings\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "t0 = time.perf_counter()\n",
    "train_emb = compute_embeddings(train_summaries)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Train embedding time/sample: {(t1 - t0)/len(train_summaries)*1000:.2f} ms\")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "test_emb = compute_embeddings(test_summaries)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Test embedding time/sample: {(t1 - t0)/len(test_summaries)*1000:.2f} ms\")\n",
    "\n",
    "train_emb = normalize(train_emb, axis=1)\n",
    "test_emb  = normalize(test_emb, axis=1)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 11. Combine features\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "X_train = np.hstack([X_train_num, train_emb])\n",
    "X_test  = np.hstack([X_test_num,  test_emb])\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 12. Balance data\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "smote_enn = SMOTEENN(random_state=42)\n",
    "X_train, y_train = smote_enn.fit_resample(X_train, y_train)\n",
    "print(\"After SMOTE+ENN:\", np.bincount(y_train))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 13. Train & Evaluate Random Forest\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 14. Performance report\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "print(\"\\n Performance Report (Gemma + RF Enhanced):\")\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Recall   : {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1 Score : {f1_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"normal\", \"attack\"]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HUZMy44GEl2-"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test):\n",
    "    import time\n",
    "    start = time.perf_counter()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    end = time.perf_counter()\n",
    "    cls_latency = (end - start) / len(y_test) * 1000\n",
    "\n",
    "    y_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, \"predict_proba\") else None\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        \"FPR\": fpr,\n",
    "        \"ConfusionMatrix\": cm.tolist(),\n",
    "        \"Latency_cls_ms\": cls_latency\n",
    "    }\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JIHAcBiQEfzq"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "results = evaluate_model(clf, X_test, y_test)\n",
    "with open(\"/content/drive/MyDrive/Results/UNSW/Random Forest/Gemma_Results.txt\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnQEBToT_xDy"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_confusion_matrix(cm, run_name, out_dir=\"/content/drive/MyDrive/Results/UNSW/Random Forest\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    np.savetxt(os.path.join(out_dir, f\"{run_name}_cm.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_title(f\"Confusion Matrix - {run_name}\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Normal\",\"Attack\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"Normal\",\"Attack\"])\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"red\")\n",
    "\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{run_name}_cm.png\"))\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q--Pv03R_x4E"
   },
   "outputs": [],
   "source": [
    "results = evaluate_model(clf, X_test, y_test)\n",
    "\n",
    "cm = np.array(results[\"ConfusionMatrix\"])\n",
    "save_confusion_matrix(cm, run_name=\"Gemma_Results\")\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Results/UNSW/Random Forest/Gemma_Results.txt\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPRUgGzWrJeJVOz4xUBqcLL",
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1mF9zyS2uvfetFrHPPk6zD3Rkre_cQHge",
     "timestamp": 1759401144334
    },
    {
     "file_id": "12QRLi00ODRZrpG9ZTWCmtDQ3hGr02fWW",
     "timestamp": 1759215426652
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
