{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hujztC_1Li0R"
   },
   "outputs": [],
   "source": [
    "# ─────────────────────────────────────────────────────────────\n",
    "# 0. Imports\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, precision_score, recall_score, f1_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder, normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import LlamaTokenizer, LlamaModel\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import torch\n",
    "import time\n",
    "from google.colab import drive\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 1. Mount Google Drive\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 2. Load UNSW-NB15 train/test\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "unsw_train_path = \"/content/drive/MyDrive/Research Project/UNSW_NB15_training-set.csv\"\n",
    "unsw_test_path  = \"/content/drive/MyDrive/Research Project/UNSW_NB15_testing-set.csv\"\n",
    "\n",
    "unsw_train = pd.read_csv(unsw_train_path)\n",
    "unsw_test  = pd.read_csv(unsw_test_path)\n",
    "\n",
    "print(\"UNSW-NB15 columns:\", list(unsw_train.columns)[:10], \"... total:\", len(unsw_train.columns))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 3. Binary labels\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "if \"label\" not in unsw_train.columns:\n",
    "    def _binlab(df):\n",
    "        if \"attack_cat\" in df.columns:\n",
    "            return (df[\"attack_cat\"].astype(str).str.lower() != \"normal\").astype(int)\n",
    "        raise ValueError(\"Missing 'label' or 'attack_cat'\")\n",
    "    unsw_train[\"label\"] = _binlab(unsw_train)\n",
    "    unsw_test[\"label\"]  = _binlab(unsw_test)\n",
    "else:\n",
    "    unsw_train[\"label\"] = (unsw_train[\"label\"] != 0).astype(int)\n",
    "    unsw_test[\"label\"]  = (unsw_test[\"label\"]  != 0).astype(int)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 4. Feature engineering\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "for df in [unsw_train, unsw_test]:\n",
    "    if \"sbytes\" in df.columns and \"dbytes\" in df.columns:\n",
    "        df[\"bytes_ratio\"] = df[\"sbytes\"] / (df[\"dbytes\"] + 1)\n",
    "        df[\"log_sbytes\"] = np.log1p(df[\"sbytes\"])\n",
    "        df[\"log_dbytes\"] = np.log1p(df[\"dbytes\"])\n",
    "    if \"dur\" in df.columns:\n",
    "        df[\"log_dur\"] = np.log1p(df[\"dur\"])\n",
    "    if \"spkts\" in df.columns and \"dpkts\" in df.columns:\n",
    "        df[\"pkt_ratio\"] = df[\"spkts\"] / (df[\"dpkts\"] + 1)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 5. Encode categorical features\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "categorical_cols = [c for c in [\"proto\", \"service\", \"state\"] if c in unsw_train.columns]\n",
    "for col in categorical_cols:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([unsw_train[col].astype(str), unsw_test[col].astype(str)], axis=0)\n",
    "    le.fit(combined)\n",
    "    unsw_train[col] = le.transform(unsw_train[col].astype(str))\n",
    "    unsw_test[col]  = le.transform(unsw_test[col].astype(str))\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 6. Numeric columns\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "drop_cols = [\"srcip\", \"dstip\", \"label\", \"attack_cat\"]\n",
    "num_cols = [c for c in unsw_train.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(unsw_train[c])]\n",
    "\n",
    "X_train_num = unsw_train[num_cols].values\n",
    "X_test_num  = unsw_test[num_cols].values\n",
    "y_train = unsw_train[\"label\"].values\n",
    "y_test  = unsw_test[\"label\"].values\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 7. Flow summaries for Gemma\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "def make_summary(row):\n",
    "    g = row.get\n",
    "    srcbytes = g(\"sbytes\", g(\"srcbytes\", 0))\n",
    "    dstbytes = g(\"dbytes\", g(\"dstbytes\", 0))\n",
    "    dur      = g(\"dur\", 0.0)\n",
    "    proto    = g(\"proto\", \"NA\")\n",
    "    service  = g(\"service\", \"NA\")\n",
    "    state    = g(\"state\", \"NA\")\n",
    "    sport    = g(\"sport\", g(\"sport\", -1))\n",
    "    dsport   = g(\"dsport\", g(\"dsport\", -1))\n",
    "    return (f\"Flow: src_bytes={srcbytes}, dst_bytes={dstbytes}, \"\n",
    "            f\"duration={float(dur):.2f}s, proto={proto}, service={service}, \"\n",
    "            f\"state={state}, sport={sport}, dsport={dsport}\")\n",
    "\n",
    "train_summaries = unsw_train.apply(make_summary, axis=1).tolist()\n",
    "test_summaries  = unsw_test.apply(make_summary, axis=1).tolist()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 8. Load Gemma model\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "login(token=\"your token hugging face\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    use_auth_token=True\n",
    ")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = LlamaModel.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    use_auth_token=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    output_hidden_states=True\n",
    ").to(device)\n",
    "model.eval()\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 9. Embedding function\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "class SummaryDataset(Dataset):\n",
    "    def __init__(self, summaries): self.summaries = summaries\n",
    "    def __len__(self): return len(self.summaries)\n",
    "    def __getitem__(self, idx): return self.summaries[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=128)\n",
    "\n",
    "def compute_embeddings(summaries, batch_size=32):\n",
    "    ds = SummaryDataset(summaries)\n",
    "    loader = DataLoader(ds, batch_size=batch_size, collate_fn=collate_fn)\n",
    "    all_embs = []\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc=\"Embedding with LLama\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            last_hidden = outputs.hidden_states[-1]\n",
    "            pooled = last_hidden[:, 0, :].cpu().numpy()  # CLS pooling\n",
    "            all_embs.append(pooled)\n",
    "    return np.vstack(all_embs)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 10. Compute embeddings\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "t0 = time.perf_counter()\n",
    "train_emb = compute_embeddings(train_summaries)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Train embedding time/sample: {(t1 - t0)/len(train_summaries)*1000:.2f} ms\")\n",
    "\n",
    "t0 = time.perf_counter()\n",
    "test_emb = compute_embeddings(test_summaries)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Test embedding time/sample: {(t1 - t0)/len(test_summaries)*1000:.2f} ms\")\n",
    "\n",
    "train_emb = normalize(train_emb, axis=1)\n",
    "test_emb  = normalize(test_emb, axis=1)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 11. PCA (reduce 768-dim to 100)\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# print(\"\\nApplying PCA to embeddings...\")\n",
    "# pca = PCA(n_components=100, random_state=42)\n",
    "# train_emb_reduced = pca.fit_transform(train_emb)\n",
    "# test_emb_reduced  = pca.transform(test_emb)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 12. Combine features\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "X_train = np.hstack([X_train_num, train_emb])\n",
    "X_test  = np.hstack([X_test_num, test_emb])\n",
    "print(\"Final training shape:\", X_train.shape)\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 13. Train & Evaluate LLama\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "clf = ExtraTreesClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=5,\n",
    "    class_weight=\"balanced\",\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nTraining ExtraTreesClassifier model...\")\n",
    "t0 = time.perf_counter()\n",
    "clf.fit(X_train, y_train)\n",
    "t1 = time.perf_counter()\n",
    "print(f\"Training time: {(t1 - t0):.2f}s\")\n",
    "\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "# 14. Evaluation\n",
    "# ─────────────────────────────────────────────────────────────\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"\\n Performance Report (LLama + ExtraTree):\")\n",
    "print(f\"Accuracy : {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n",
    "print(f\"Recall   : {recall_score(y_test, y_pred):.3f}\")\n",
    "print(f\"F1 Score : {f1_score(y_test, y_pred):.3f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"normal\", \"attack\"]))\n",
    "print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OP9RUwZvMJTn"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def evaluate_model(clf, X_test, y_test):\n",
    "    import time\n",
    "    start = time.perf_counter()\n",
    "    y_pred = clf.predict(X_test)\n",
    "    end = time.perf_counter()\n",
    "    cls_latency = (end - start) / len(y_test) * 1000\n",
    "\n",
    "    y_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, \"predict_proba\") else None\n",
    "\n",
    "    # Confusion matrix + FPR\n",
    "    cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n",
    "\n",
    "    results = {\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred),\n",
    "        \"ROC-AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n",
    "        \"FPR\": fpr,\n",
    "        \"ConfusionMatrix\": cm.tolist(),\n",
    "        \"Latency_cls_ms\": cls_latency\n",
    "    }\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AN4woa59MKFN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def save_confusion_matrix(cm, run_name, out_dir=\"/content/drive/MyDrive/Results/UNSW/ExtraTreesClassifier\"):\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "    np.savetxt(os.path.join(out_dir, f\"{run_name}_cm.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(4,4))\n",
    "    im = ax.imshow(cm, cmap=\"Blues\")\n",
    "    ax.set_title(f\"Confusion Matrix - {run_name}\")\n",
    "    ax.set_xlabel(\"Predicted\")\n",
    "    ax.set_ylabel(\"True\")\n",
    "    ax.set_xticks([0,1]); ax.set_xticklabels([\"Normal\",\"Attack\"])\n",
    "    ax.set_yticks([0,1]); ax.set_yticklabels([\"Normal\",\"Attack\"])\n",
    "\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"red\")\n",
    "\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{run_name}_cm.png\"))\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LDSCiSRkMQc_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "results = evaluate_model(clf, X_test, y_test)\n",
    "\n",
    "cm = np.array(results[\"ConfusionMatrix\"])\n",
    "save_confusion_matrix(cm, run_name=\"LLama_DT\")\n",
    "\n",
    "with open(\"/content/drive/MyDrive/Results/UNSW/ExtraTreesClassifier/LLama_Results.txt\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "1fWdMIQc4q0yXmWc1fxwiOUhdTUrqA0Kn",
     "timestamp": 1759424434222
    },
    {
     "file_id": "1nYUhqwWiKxX2iUQoo_Zakinv3jV7VjBO",
     "timestamp": 1759420687572
    },
    {
     "file_id": "1L3xiCZK4Xv9oM42lAAeflY-ABJFvIfmd",
     "timestamp": 1759215411430
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
