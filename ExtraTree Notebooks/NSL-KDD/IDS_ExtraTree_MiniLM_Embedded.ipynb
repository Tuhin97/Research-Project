{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1eSzMrsZBhA8pcwCRkGOAHYwtkZHoxbZt","timestamp":1760247599208},{"file_id":"1_11PB5-pQp7vtEdNBROMeALVBZQEomRU","timestamp":1759424004270},{"file_id":"1dWR-PFFxXOWek0buOgKOfYaJJvwM1e6p","timestamp":1759409631871},{"file_id":"1O3cOXgmr5nE5qmFZwJmYtxkrMriN62Jv","timestamp":1759215418576}],"machine_shape":"hm","gpuType":"A100","authorship_tag":"ABX9TyMq8SVvVZplqbd3GFcZeImX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# ─────────────────────────────────────────────────────────────\n","# 0. Imports\n","# ─────────────────────────────────────────────────────────────\n","import os\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from sklearn.metrics import (\n","    classification_report, confusion_matrix,\n","    accuracy_score, precision_score, recall_score, f1_score\n",")\n","from sklearn.preprocessing import LabelEncoder, normalize\n","from sentence_transformers import SentenceTransformer\n","from sklearn.ensemble import ExtraTreesClassifier\n","import torch, time\n","from google.colab import drive\n","\n","# ─────────────────────────────────────────────────────────────\n","# 1. Mount Google Drive\n","# ─────────────────────────────────────────────────────────────\n","drive.mount('/content/drive')\n","\n","# ─────────────────────────────────────────────────────────────\n","# 2. Load NSL-KDD train/test\n","# ─────────────────────────────────────────────────────────────\n","train_path = \"/content/drive/MyDrive/Research Project/KDDTrain+.txt\"\n","test_path  = \"/content/drive/MyDrive/Research Project/KDDTest+.txt\"\n","\n","columns = [\n","    \"duration\",\"protocol_type\",\"service\",\"flag\",\"src_bytes\",\"dst_bytes\",\n","    \"land\",\"wrong_fragment\",\"urgent\",\"hot\",\"num_failed_logins\",\"logged_in\",\n","    \"num_compromised\",\"root_shell\",\"su_attempted\",\"num_root\",\"num_file_creations\",\n","    \"num_shells\",\"num_access_files\",\"num_outbound_cmds\",\"is_host_login\",\n","    \"is_guest_login\",\"count\",\"srv_count\",\"serror_rate\",\"srv_serror_rate\",\n","    \"rerror_rate\",\"srv_rerror_rate\",\"same_srv_rate\",\"diff_srv_rate\",\n","    \"srv_diff_host_rate\",\"dst_host_count\",\"dst_host_srv_count\",\n","    \"dst_host_same_srv_rate\",\"dst_host_diff_srv_rate\",\"dst_host_same_src_port_rate\",\n","    \"dst_host_srv_diff_host_rate\",\"dst_host_serror_rate\",\"dst_host_srv_serror_rate\",\n","    \"dst_host_rerror_rate\",\"dst_host_srv_rerror_rate\",\n","    \"attack_type\",\"difficulty\"\n","]\n","\n","train_df = pd.read_csv(train_path, header=None, names=columns)\n","test_df  = pd.read_csv(test_path,  header=None, names=columns)\n","print(f\"Train shape: {train_df.shape}, Test shape: {test_df.shape}\")\n","\n","# ─────────────────────────────────────────────────────────────\n","# 3. Binary labels\n","# ─────────────────────────────────────────────────────────────\n","train_df[\"label\"] = (train_df[\"attack_type\"] != \"normal\").astype(int)\n","test_df[\"label\"]  = (test_df[\"attack_type\"]  != \"normal\").astype(int)\n","\n","# ─────────────────────────────────────────────────────────────\n","# 4. Encode categorical features\n","# ─────────────────────────────────────────────────────────────\n","categorical_cols = [\"protocol_type\", \"service\", \"flag\"]\n","for col in categorical_cols:\n","    le = LabelEncoder()\n","    combined = pd.concat([train_df[col].astype(str), test_df[col].astype(str)], axis=0)\n","    le.fit(combined)\n","    train_df[col] = le.transform(train_df[col].astype(str))\n","    test_df[col]  = le.transform(test_df[col].astype(str))\n","\n","# ─────────────────────────────────────────────────────────────\n","# 5. Select numeric columns\n","# ─────────────────────────────────────────────────────────────\n","drop_cols = [\"attack_type\", \"difficulty\", \"label\"]\n","num_cols = [c for c in train_df.columns if c not in drop_cols and pd.api.types.is_numeric_dtype(train_df[c])]\n","X_train_num = train_df[num_cols].values\n","X_test_num  = test_df[num_cols].values\n","y_train = train_df[\"label\"].values\n","y_test  = test_df[\"label\"].values\n","print(f\"Using {len(num_cols)} numeric features\")\n","\n","# ─────────────────────────────────────────────────────────────\n","# 6. Flow-style textual summaries\n","# ─────────────────────────────────────────────────────────────\n","def make_summary(row):\n","    g = row.get\n","    proto  = g(\"protocol_type\",\"NA\")\n","    serv   = g(\"service\",\"NA\")\n","    flag   = g(\"flag\",\"NA\")\n","    src_b  = g(\"src_bytes\",0)\n","    dst_b  = g(\"dst_bytes\",0)\n","    dur    = g(\"duration\",0.0)\n","    serror = g(\"serror_rate\",0.0)\n","    rerror = g(\"rerror_rate\",0.0)\n","    count  = g(\"count\",0)\n","    srv_ct = g(\"srv_count\",0)\n","    return (f\"Connection: proto={proto}, service={serv}, flag={flag}, \"\n","            f\"duration={dur:.2f}s, src_bytes={src_b}, dst_bytes={dst_b}, \"\n","            f\"serror_rate={serror:.2f}, rerror_rate={rerror:.2f}, \"\n","            f\"count={count}, srv_count={srv_ct}\")\n","\n","train_summaries = train_df.apply(make_summary, axis=1).tolist()\n","test_summaries  = test_df.apply(make_summary, axis=1).tolist()\n","print(\"Sample summary:\", train_summaries[0][:120], \"...\")\n","\n","# ─────────────────────────────────────────────────────────────\n","# 7. Load MiniLM\n","# ─────────────────────────────────────────────────────────────\n","print(\"\\nLoading MiniLM model...\")\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\", device=device)\n","\n","# ─────────────────────────────────────────────────────────────\n","# 8. Compute embeddings\n","# ─────────────────────────────────────────────────────────────\n","def embed_texts(texts, batch_size=32):\n","    embs = []\n","    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding with MiniLM\"):\n","        batch = texts[i:i+batch_size]\n","        emb = model.encode(batch, convert_to_numpy=True, normalize_embeddings=True, show_progress_bar=False)\n","        embs.append(emb)\n","    return np.vstack(embs)\n","\n","t0 = time.perf_counter(); train_emb = embed_texts(train_summaries)\n","t1 = time.perf_counter(); print(f\"Train embedding time/sample: {(t1-t0)/len(train_summaries)*1000:.2f} ms\")\n","\n","t0 = time.perf_counter(); test_emb = embed_texts(test_summaries)\n","t1 = time.perf_counter(); print(f\"Test embedding time/sample: {(t1-t0)/len(test_summaries)*1000:.2f} ms\")\n","\n","# ─────────────────────────────────────────────────────────────\n","# 9. Combine numeric + text embeddings\n","# ─────────────────────────────────────────────────────────────\n","X_train = np.hstack([X_train_num, train_emb])\n","X_test  = np.hstack([X_test_num,  test_emb])\n","print(\"Final training shape:\", X_train.shape)\n","\n","# ─────────────────────────────────────────────────────────────\n","# 10. Train & Evaluate Extra Trees\n","# ─────────────────────────────────────────────────────────────\n","print(\"\\nTraining Extra Trees model...\")\n","clf = ExtraTreesClassifier(\n","    n_estimators=300,\n","    max_depth=None,\n","    min_samples_leaf=5,\n","    class_weight=\"balanced\",\n","    n_jobs=-1,\n","    random_state=42\n",")\n","t0 = time.perf_counter(); clf.fit(X_train, y_train)\n","t1 = time.perf_counter(); print(f\"Training time: {(t1-t0):.2f}s\")\n","\n","# ─────────────────────────────────────────────────────────────\n","# 11. Evaluation\n","# ─────────────────────────────────────────────────────────────\n","y_pred = clf.predict(X_test)\n","print(\"\\nPerformance Report (MiniLM + Extra Trees):\")\n","print(f\"Accuracy : {accuracy_score(y_test, y_pred):.3f}\")\n","print(f\"Precision: {precision_score(y_test, y_pred):.3f}\")\n","print(f\"Recall   : {recall_score(y_test, y_pred):.3f}\")\n","print(f\"F1 Score : {f1_score(y_test, y_pred):.3f}\")\n","print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=[\"normal\",\"attack\"]))\n","print(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n"],"metadata":{"id":"BvJBnyaG24Vp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import roc_auc_score\n","\n","def evaluate_model(clf, X_test, y_test):\n","    import time\n","    start = time.perf_counter()\n","    y_pred = clf.predict(X_test)\n","    end = time.perf_counter()\n","    cls_latency = (end - start) / len(y_test) * 1000  # ms per sample\n","\n","    y_proba = clf.predict_proba(X_test)[:,1] if hasattr(clf, \"predict_proba\") else None\n","\n","    # Confusion matrix + FPR\n","    cm = confusion_matrix(y_test, y_pred, labels=[0,1])\n","    tn, fp, fn, tp = cm.ravel()\n","    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0.0\n","\n","    results = {\n","        \"Accuracy\": accuracy_score(y_test, y_pred),\n","        \"Precision\": precision_score(y_test, y_pred),\n","        \"Recall\": recall_score(y_test, y_pred),\n","        \"F1\": f1_score(y_test, y_pred),\n","        \"ROC-AUC\": roc_auc_score(y_test, y_proba) if y_proba is not None else None,\n","        \"FPR\": fpr,\n","        \"ConfusionMatrix\": cm.tolist(),\n","        \"Latency_cls_ms\": cls_latency\n","    }\n","    return results\n"],"metadata":{"id":"OXUu3DinXiKO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","\n","def save_confusion_matrix(cm, run_name, out_dir=\"/content/drive/MyDrive/Results/NSB/ExtraTreesClassifier\"):\n","    os.makedirs(out_dir, exist_ok=True)\n","    # Save CSV\n","    np.savetxt(os.path.join(out_dir, f\"{run_name}_cm.csv\"), cm, fmt=\"%d\", delimiter=\",\")\n","\n","    # Save image\n","    fig, ax = plt.subplots(figsize=(4,4))\n","    im = ax.imshow(cm, cmap=\"Blues\")\n","    ax.set_title(f\"Confusion Matrix - {run_name}\")\n","    ax.set_xlabel(\"Predicted\")\n","    ax.set_ylabel(\"True\")\n","    ax.set_xticks([0,1]); ax.set_xticklabels([\"Normal\",\"Attack\"])\n","    ax.set_yticks([0,1]); ax.set_yticklabels([\"Normal\",\"Attack\"])\n","\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            ax.text(j, i, cm[i,j], ha=\"center\", va=\"center\", color=\"red\")\n","\n","    fig.colorbar(im, ax=ax)\n","    fig.tight_layout()\n","    fig.savefig(os.path.join(out_dir, f\"{run_name}_cm.png\"))\n","    plt.close(fig)\n"],"metadata":{"id":"s5q9Gw2tXjK4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import json\n","\n","results = evaluate_model(clf, X_test, y_test)\n","\n","cm = np.array(results[\"ConfusionMatrix\"])\n","save_confusion_matrix(cm, run_name=\"MiniLM_DT\")\n","\n","with open(\"/content/drive/MyDrive/Results/NSB/ExtraTreesClassifier/MiniLM_Results.txt\", \"w\") as f:\n","    json.dump(results, f, indent=2)\n"],"metadata":{"id":"wdmSJ4hbXv8c"},"execution_count":null,"outputs":[]}]}